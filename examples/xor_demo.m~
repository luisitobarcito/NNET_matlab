% XOR demonstration for NNET_matlab
clear all
close all
clc

%% Create data set 
N = 1000;
X = mvnrnd([0, 0], eye(2), N);
bin_idx = rand(N, 2) > 0.5;

X = X + bin_idx*4;
labels =  xor(bin_idx(:,1), bin_idx(:,2));
% little visualization
gscatter(X(:,1), X(:,2), labels)
X = X';
labels = labels'; 
%% Create network
addpath('../../NNET_matlab');
run ../load_nnet_basic.m;

in_dim = size(X, 1);
hid_dim = 6;
out_dim = size(labels, 1);

net.layers(1) = layer;
net.layers(1).W = randn(hid_dim, in_dim)/sqrt(in_dim);
net.layers(1).b = zeros(hid_dim, 1);
net.layers(1).f = relu;
net.layers(1).f_prime = relu_prime;
net.layers(1).delta_W = zeros(hid_dim, in_dim);
net.layers(1).delta_b = zeros(hid_dim, 1);
net.layers(1).grad_W = zeros(hid_dim, in_dim);
net.layers(1).grad_b = zeros(hid_dim, 1);

net.layers(2) = layer;
net.layers(2).W = randn(out_dim, hid_dim)/sqrt(hid_dim);
net.layers(2).b = zeros(out_dim, 1);
net.layers(2).f = logistic;
net.layers(2).f_prime = logistic_prime;
net.layers(2).delta_W = zeros(out_dim, hid_dim);
net.layers(2).delta_b = zeros(out_dim, 1);
net.layers(2).grad_W = zeros(out_dim, hid_dim);
net.layers(2).grad_b = zeros(out_dim, 1);

%% Train the network
%% And Now the algorithm finally
batchsize = 100;
n_epochs = 100;
stepsize = 0.001;
[X_batches] = createMiniBatches(X, batchsize);
n_batches = size(X_batches.data, 3);
total_cost = zeros(n_batches, n_epochs);
for iEpc = 1:n_epochs
    fprintf('Epoch  %d\n',iEpc);
    for iBtch = 1 : n_batches
        
        net.layers = propagateForward(net.layers, X_batches.data(:,:, iBtch));
        % sample from code distribution
        Pred_label = net.layers(end).X_out;
        Pred_err = Pred_label - X_batches.labels(:,:,iBtch);
        total_cost(iBtch, iEpc) = (1/batchsize)*sum(-X_batches.labels(:,:,iBtch).*log(Pred_label) - (1-X_batches.labels(:,:,iBtch)).*log(1- Pred_label));
        diff_cost = (1/batchsize)*Pred_err./(Pred_label.*(1-Pred_label));
        % backpropagate the gradients
        net.layers = propagateBackward(net.layers, diff_cost);
        % compute updates for decoder networks
        net.layers = updateParamters(datanet.decoder.layers, stepsize, 'sgd');
        % take care of tied weights
        for iLyr = 1 : n_layers
            W_data_temp = (datanet.decoder.layers(iLyr).W' + datanet.encoder.layers(end - iLyr + 1).W)/2;
            datanet.decoder.layers(iLyr).W = W_data_temp';
            datanet.encoder.layers(end - iLyr + 1).W = W_data_temp;
            W_dist_temp = (distnet.decoder.layers(iLyr).W' + distnet.encoder.layers(end - iLyr + 1).W)/2;
            distnet.decoder.layers(iLyr).W = W_dist_temp';
            distnet.encoder.layers(end - iLyr + 1).W = W_dist_temp;
        end
        
        if mod(iBtch, 10)  == 0
            subplot(321)
            imshow(reshape(X_batches.data(:,1,iBtch), [28,28])', [])
            subplot(322)
            imshow(reshape(datanet.decoder.layers(end).X_out(:,1), [28,28])', []);
            distnet.decoder.layers = propagateForward(distnet.decoder.layers, datanet.encoder.layers(end).X_out);
            X_code = distnet.decoder.layers(end).X_out;
            subplot(3,2,[3,4,5,6])
            gscatter(X_code(1, :), X_code(2, :), X_batches.targets(1,:,iBtch) )
            xlim([-4 4])
            ylim([-4 4])
            drawnow;
            pause(0.01)
        end
    end
    fprintf('Cost is %f,\nMMD %f, data_rec %f, dist_rec %f\n', mean(total_cost(:, iEpc)), mean(mmd_val(:, iEpc)), mean(data_rec_cost(:, iEpc)), mean(dist_rec_cost(:, iEpc)));
end